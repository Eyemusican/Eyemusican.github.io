---
Title : DAM101 Flipped class 1
categories : [DAM201, Flipped_Class1]
tags : [DAM101]
---

### What I learned during flipped class!

---

Hello there, I'm Tenzin Namgay, and this is my  journal entry about what I learned during the flipped class. I hope you are excited to learn about Deep learning fundamentals, which is the topic for this journal. Without any further, let's get started.

Firstly, let us understand what Artifical Intelligence is before going into depth.
AI is started by Arthur Samuel at ninteenth century, it is a machine and has ability to mimic human behaviour.
In this modern world, AI learns, improves and handle complex problem like converting text-to-speech, Image detection using python and many more.
With this understanding, let us differentiate between deep-learning and machine-learning.
Deep-learning uses complex mathametical algorithm in large dataset to detect patterns, whereas AI uses nerual network that learns dataset(just like human brain but does the work by machine).AI handles more data than machine-learning, hence it handles complex problem as mentioned above. In simple terms, from my understanding, AI aims in mimicking human behaviour, whereas ML learns data and improve performance by using algorithms.

Now, lets talk about neurons in AI, they are computational units. Form my understanding, their function is to process and transmittt information by receving inputs, applying weights to these inputs,suming them and applying activation function to produce an output.
The result for the above procedure is they learn and make decisions,mimicking human behaviour.
The simplest form of neural network consisting only perceptrons is called Single Layer Perceptron.
To talk about more, they are not capable of solving complex problems and can only learn to classify data. The result for this perceptron is binnary, classification decision.

Lastly, I will jot down variables associated with modelling a single neuron;
a. Input(x)--> symbol
b. Weights(w)
c. Bias(b)
d. Weighted sum(z)
e. Activation function(t)
f. Output(y)

To give a simple description: A signal is received by a neuron from other neurons. Each signal (input) has a weight that contributes to the neuron's output. The bias allows the neuron to capture patterns even when the inputs are zero. Next, multiplying and adding the input weights together and adding the bias term is done at the weighted sum. Then, the activation function allows the neuron to learn complex patterns and finally determines the output.










